%Establish the context and importance of the topic in this text.

%This degree project was carried out within the field of deep learning and generative modeling.

% TODO Ge exempel på tillämningar
Generative models are essential for a wide range of applications within Data Science and Machine Learning. Unlike discriminative models, generative models are capable of reproducing the phenomenon they are modelling enabling them to be used for synthetic data generation. This have been shown to be successful for low dimensional relational data, for an example \textcite{patki2016synthetic} showed that generative models could be used as a drop in replacement for a relational data base. However achieving similar results on large high dimensional data sets have proven to be more challenging.

Until recently, there has been little progress in developing generative models that can scale beyond small low-dimensional data sets. Therefore the most refined generative models of today are intractable to apply on large high-dimensional data sets. The past few years have seen major advances in deep learing. These advances allowed a new class of scalable generative models to emerge. Namely, deep generative models. Of these models, a significant amount of attention have been directed at \acrfull{gans} which are capable of generating highly realistic samples of complicated data distributions.

\acrlong{gans} were first introduced by \textcite{goodfellow2014generative} in 2014 as a method of generating realistic samples of a data distribution. The state-of-the-art \acrshort{gans} of today are capable of generating diverse sets of high resolution images \cite{karras2017progressive}. Adversarial methods have also helped improve existing data sets for gaze estimation \cite{shrivastava2016learning,sela2017gazegan}. 
%A big interest in researching \acrshort{gans} emerges from the unsupervised nature of the method. By utilizing \acrshort{gans} large unannotated data sets can be leveraged to boost existing models. An example of this is using \acrshort{gans} semi-supervised learning as proposed by Goodfellow et al. in the original article \cite{goodfellow2014generative}.

A considerable amount of literature has been published on how to train \acrshort{gans}. These studies consider convergence issues related to training \acrshort{gans} and the quality of the generated sample. A search of the literature revealed few studies which attempts to train new networks on the generated samples. This project provides an important opportunity to advance the understanding of these possibilities.

\section{Problem statement}
This thesis intends to determine the extent to which deep generative models can act as a drop-in replacement for existing data sets. The question asked is

\begin{itemize}
    %\item \textit{Can deep generative models be applied to generate synthetic data sets in such a way that neural networks trained on the synthetic data set exhibits performance comparable to the performance of the same models when trained on real data?}
    \item \textit{Can deep generative models be applied to generate synthetic data sets that can be used to boost the performance of existing discriminative models?}
\end{itemize}

%\section{Motivation for synthetic data generation}
%Synthetic data sets \textcite{patki2016synthetic}. Self training in semi-supervised learning \parencite{wuliu2017selftrainsemisup}. And more.

\section{Scope and objectives}
This study is unable to encompass the entire field of deep generative models as well as all possible data sources. Therefore, this study examines the previously stated question through a case study within semantic segmentation for pupil regression. Special emphasis is put on examinating generating synthetic data sets for this task through \acrlong{gans}.

\section{Societal and ethical conciderations}
In the new global economy, extracting knowledge from data has become a central issue for a wide range of industries. Generative models play an important role in this process. As more industries move towards a data driven business model, the scalability and applicability of certain generative models in new problem domains are increasingly interesting. The outcomes of this project are especially relevant to industries that utilize large sets of images for supervised learning as it provides valuable insights of an approach for learning hidden structures in this type of data.

Another source of interest from the economic perspective emerges from the fact that manually annotated data is expensive and requires large amounts of human labour. Increasing the effectiveness of the existing data might reduce the need for extending the size of the annotated data sets in some cases. 

In the case of very large databases, both economical and ecological benefits exist. A genertative model requires only a small fraction of the storage space that the full dataset needs. Therefore replacing databases with generative models has the potential of saving vast amounts of resources and thus being a both cheaper and more ecological alternative to huge data stores when applicable.  

% Ethical conciderations - replacing with generative models anonymous but not anonymous
This approach comes with an ethical issue though. When the data consists of personal information, data belonging to a specific individual can easily be removed from a conventional data set. However there is no obvious way to delete the impact of that data onto a trained generative model. It could be argued that by training the generative model, the data have already been anonymized. However the extent to which this is true depends on the specific applied model. There are many aspects to concider regarding personal integrity and generative modelling, and a full discussion is well beyond the scope of this project. Due to this and the novelty of the methods, they might provide a loophole to circumvent several data protection laws. This risk is assumed quite low given the current state-of-the-art, allowing this thesis to be published but any further incremental research within the field should be aware of it.

\section{Thesis overview}
Chapter 2 provides and overview of \acrlong{gans} and related methods. The notation used throughout the thesis is establised in section 2.1. In Chapter 3, the methods emplyed to answer the problem statement is presented. The experimental results are presented in Chapter 4 and discussed in Chapter 5. In Chapter 5, the final conclusion is stated.

%\subsection{Semi-supervised learning}

%\subsection{Data compression}

