\section{Generative adversarial networks}
\acrfull{gans} are a family generative models proposed by \textcite{goodfellow2014generative}. \acrshort{gans} consists of a data domain $\mathcal{D}$ in some space $\mathcal{X}$, a latent space $\mathcal{Z}$ and two neural networks, a generator $G$ and a discriminator $D$. The generator is a mapping from $\mathcal{Z}$ to $\mathcal{X}$, $G: \mathcal{Z} \rightarrow \mathcal{X}$. The discriminator is a binary classifier $D: \mathcal{X} \rightarrow [0, 1]$. 

The objective of the discriminator is to classify elements in $\mathcal{X}$ as either members or not members of $\mathcal{D}$. Members of $\mathcal{D}$ are usually referred to as real samples, and non-members as fake samples. The objective of the generator is to fool the discriminator by mapping elements in $\mathcal{Z}$ to the subspace of $\mathcal{X}$ that is classified as real. By introducing the adversarial loss function
\begin{equation}
    \mathcal{L}(x, z) = -log(D(x)) + log(D(G(z)))
\end{equation}
minimax stuff... todo

The generator can bee seen to represent a probability distribution $p_G$ on $\mathcal{X}$ according to $p_G(x) = p_Z(G^{-1}(x))$, assuming a distribution $p_Z$ on $\mathcal{Z}$. In practice $G^{-1}$ is intractable to compute, whereby explicit probabilities are seldom acquired through \acrshort{gans}.


\subsection{Variations on generative models}

\section{Other generative models}

\subsection{Variatonal Autoencoders}

\section{Image-to-image transforms}

\section{Common architectures}

\section{Normalization techniques}

\section{Related work}




