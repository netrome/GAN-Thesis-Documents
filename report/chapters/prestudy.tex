\section{Concepts and Terminology}


\section{\acrlong{gans}}
\acrfull{gans} are a family generative models proposed by \textcite{goodfellow2014generative}. The framework for training \acrshort{gans} consists of a data set $\dataset$ with elements in a domain $X$, a latent space $\latentspace$ and two neural networks, a generator $G$ and a discriminator $D$. The generator is a mapping from $\latentspace$ to $X$, $G: \latentspace \rightarrow X$. The discriminator is a binary classifier $D: X \rightarrow [0, 1]$. 

The objective of the discriminator is to classify elements in $X$ as either members or not members of $\dataset$. Members of $\dataset$ are usually referred to as real samples since they are part of the data set, and non-members are referred to as fake samples. The objective of the generator is to fool the discriminator by mapping elements in $\latentspace$ to the subspace of $X$ that is classified as real. 

The generator can bee viewed as representing a probability distribution $p_G$ on $X$ according to $p_G(x) = p_\latentspace(G^{-1}(x))$, assuming a distribution $p_\latentspace$ on $\latentspace$. In practice $G^{-1}$ is intractable to compute, whereby explicit probabilities are seldom acquired through \acrshort{gans}. Moreover, the elements of $\dataset$ can be seen as outcomes of a probability distribution $p_\dataset$ corresponding to the probability of the occurence of a specific data point. Using this formulation the objective of the \acrshort{gan} training can be formulated as a minimax game 
\begin{equation}
    \min_D \max_G J^{D}(G, D)
\end{equation}
where $J^D(G, D)$ is the discriminator cost function from \parencite{goodfellow2016nips},
\begin{equation}
    J^D(G, D) = -\frac{1}{2}\mathbb{E}_{x \sim p_\dataset, z \sim p_\latentspace}\left[\log(D(x)) - \log(1 - D(G(z))) \right].
\end{equation}
%\begin{equation}
%    \mathcal{L}(x_1, x_2) = -\log(D(x)) + \log(D(x_2)), \quad \begin{cases} x \in X \\ z \in \latentspace \end{cases}.
%\end{equation}


\subsection{\acrshort{gan} variations}
\subsubsection{BiGAN}
Can utilize the latent space.
\subsubsection{ACGAN}
Generates nice images. \textcite{odena2016conditional} Distribution is skewed. \textcite{shuac2017acganisbad}.
\subsubsection{WGAN}
\subsubsection{BEGAN}
\subsubsection{DRAGAN}
\subsubsection{Progressive GAN}

\section{\acrlong{vaes}}
Another popular family of generative models besides \acrshort{gans} are \acrfull{vaes}. \acrlong{vaes} were first introduced by \textcite{kingma2013auto} as a scalable approach for stochastic variational inference. The model consists of an i.i.d. data set $\dataset$ and a continous latent space $\latentspace$. Each data point $\ve{x} \in \dataset$ 

\section{Semi-supervised learning}
Formulate problem, go through some apporaches.

Improved GAN, go through vanilla version and combination with self training

\section{Image-to-image transforms}

\section{Common architectures}

\section{Normalization techniques}

\section{Related work}
Hmm, värt att nämna? 






