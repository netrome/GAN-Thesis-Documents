\section{Generative adversarial networks}
\acrfull{gans} are a family generative models proposed by \textcite{goodfellow2014generative}. The framework for training \acrshort{gans} consists of data in a domain $X$, a latent space $Z$ and two neural networks, a generator $G$ and a discriminator $D$. The generator is a mapping from $Z$ to $X$, $G: Z \rightarrow X$. The discriminator is a binary classifier $D: X \rightarrow [0, 1]$. 

The objective of the discriminator is to classify elements in $X$ as either members or not members of $D$. Members of $D$ are usually referred to as real samples, and non-members as fake samples. The objective of the generator is to fool the discriminator by mapping elements in $Z$ to the subspace of $X$ that is classified as real. By introducing the adversarial loss function
\begin{equation}
    \mathcal{L}(x, z) = -log(D(x)) + log(D(G(z)))
\end{equation}
minimax stuff... todo

The generator can bee seen to represent a probability distribution $p_G$ on $X$ according to $p_G(x) = p_Z(G^{-1}(x))$, assuming a distribution $p_Z$ on $Z$. In practice $G^{-1}$ is intractable to compute, whereby explicit probabilities are seldom acquired through \acrshort{gans}.


\subsection{Variations on generative models}

\section{Other generative models}

\subsection{Variatonal Autoencoders}

\section{Image-to-image transforms}

\section{Common architectures}

\section{Normalization techniques}

\section{Related work}




