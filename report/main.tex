\documentclass{kththesis}

\usepackage{blindtext} % This is just to get some nonsense text in this template, can be safely removed

\usepackage{csquotes} % Recommended by biblatex
\usepackage{biblatex}
\addbibresource{references.bib} % The file containing our references, in BibTeX format

% Own packages -------------------------------------
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{svg}
\graphicspath{ {images/} }
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{soul}
\usepackage[colorinlistoftodos, textsize=tiny]{todonotes}

\sisetup{round-mode=figures,round-precision=3}
\sisetup{detect-weight=true}
\usepackage[acronym]{glossaries}
\usepackage{algorithm,algorithmic}% http://ctan.org/pkg/algorithms
\makeglossaries
\newacronym{gan}{GAN}{Generative Adversarial Network}
\newacronym{gans}{GANs}{Generative Adversarial Networks}
\newacronym{vae}{VAE}{Variational Autoencoder}
\newacronym{vaes}{VAEs}{Variational Autoencoders}
\newacronym{mse}{MSE}{Mean Squared Error}
\newacronym{aevb}{AEVB}{Auto Encoding Variational Bayes}
\newacronym{cnn}{CNN}{Convolutional Neural Network}
\newacronym{cnns}{CNNs}{Convolutional Neural Networks}
\newacronym{is}{IS}{Inception Score}
\newacronym{fid}{FID}{Fréchet Inception Distance}
\newacronym{wgan}{WGAN}{Wasserstein GAN}
\newacronym{aegan}{AEGAN}{Autoencoding GAN}
\newacronym{roi}{ROI}{Region Of Interest}
\newacronym{vaegan}{VAEGAN}{Variational Autoencoder + Generative Adversarial Network}
% --------------------------------------------------
% Own commands ---------

\newcommand{\todocomment}[1]{\todo[color=green!40]{#1}}
\newcommand{\todoproofread}[1]{\todo[color=red!40]{#1}}
\newcommand{\ve}[1]{\mathbf{#1}}
\newcommand{\dataset}{\mathcal{S}}
\newcommand{\latentspace}{\mathcal{Z}}
\newcommand{\dataspace}{\mathcal{X}}
\newcommand{\eqdist}{\stackrel{\text{d}}{=}}
% ----------------------

\title{Augmenting High Dimensional Data with Deep Generative Models}
\alttitle{Syntetisk Datagenerering med Djupa Neurala Nätverk för Träning och Evaluering av Pupilregressor}
\author{Mårten Nilsson}
\email{marten3@kth.se}
\supervisor{Josephine Sullivan}
\examiner{Hedvig Kjellström}
\programme{Master in Machine Learning}
\school{School of Computer Science and Communication}
\date{\today}


\begin{document}

% Frontmatter includes the titlepage, abstracts and table-of-contents
\frontmatter

\titlepage

\begin{abstract}
Data augmentation is a techinque that can be performed in various ways to improve the training of discriminative models. The recent developments in deep generative models offer new unexplored ways of augmenting existing data sets. In this thesis, a framework for augmenting annotated data sets with deep generative models is proposed together with a method for quantitatively evaluating the quality of the generated data sets. Using this framework, two data sets for pupil localization is generated with different generative models, including both well-established models and a unique model proposed for this purpose. The unique model was shown both qualitatively and quantitatively to generate the best data sets. A set of smaller experiments on standard data sets also revealed cases where this generative model could improve the performance of an existing discriminative model. The results indicate that generative models can be used to augment or replace existing data sets when training discriminative models.
  %\blindtext
\end{abstract}


\begin{otherlanguage}{swedish}
  \begin{abstract}
    Träutensilierna i ett tryckeri äro ingalunda en oviktig faktor,
    för trevnadens, ordningens och ekonomiens upprätthållande, och
    dock är det icke sällan som sorgliga erfarenheter göras på grund
    af det oförstånd med hvilket kaster, formbräden och regaler
    tillverkas och försäljas Kaster som äro dåligt hopkomna och af
    otillräckligt.
  \end{abstract}
\end{otherlanguage}

\section*{Acknowledgements}
Josephine Sullivan, Hedvig Kjellström, Alexander Davies, Family and Friends etc. 

\tableofcontents


% Mainmatter is where the actual contents of the thesis goes
\mainmatter

% Here's my thesis ---------------------------------------------------------------------------------------

\chapter{Introduction}
%Motivation: Data compresion, can express data sets with smaller space requirements. Improved consistency (better annotations). Semi-supervised learning. Implicit semi-supervised learning.
\input{chapters/intro.tex}


\chapter{Background}
\input{chapters/prestudy.tex}

\chapter{Methods}
\input{chapters/method.tex}
%Modelling annotated data: Normal GANs, image-to-image models.

%Modelling partially annotated data: Triple-GAN.

%Normalization technoques. Talk more about spectral normalization. 

%Implementation details: Highlight spectral normalziation observation (about weight matrix spectral norm in convolutional layer due to the shape). Talk about vectorization and viewing convolutions as a linear layer. 

\chapter{Results}
\input{chapters/results.tex}

\chapter{Discussion}
\input{chapters/discussion.tex}

\chapter{Conclusion}
\input{chapters/conclusion.tex}

% --------------------------------------------------------------------------------------------------------

\printbibliography[heading=bibintoc] % Print the bibliography (and make it appear in the table of contents)

\appendix

\chapter{Lots of images here}

\end{document}
